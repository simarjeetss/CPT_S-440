{
 "metadata": {
  "kernelspec": {
   "display_name": "rl_tutorial",
   "language": "python",
   "name": "rl_tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30497,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Reinforcement Learning\nReinforcement learning is a machine learning training method based on rewarding desired behaviors and/or punishing undesired ones.\n\nIn general, a reinforcement learning agent is able to perceive and interpret its environment, take actions and learn through trial and error.\n\n<img src=\"https://raw.githubusercontent.com/MandM-DataScience/chess-engine/master/images/Reinforcement_learning_diagram.svg.png\" alt=\"Reinforcement learning diagram\" width=300 height=300>\n\nThe agent takes in a representation of the environment (State) and decides which Action to take (based on what it thinks the reward will be).\n\nAt the beginning it has no idea of which Action will yield the best reward, so it will take random Actions and discover step by step which Action is the best for each State!\n\nThis is the trial and error part of the process.\n\nSo, the main components of our reinforcement learning model are:\n- State representation (how are we going to encode a chess board position into a State)\n- Action space (what possible Action can the agent make)\n- Neural network architecture (how the Agent chooses which Action to take based on the current State)\n- Reward signal (how are we going to reward/punish the agent for an Action it took)\n- Exploration strategy (how do we balance the exploration of new Actions with the exploitation of what the Agent has learned in previous steps)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## State Representation\n\nA chess board position can be efficiently stored as a list of bitboards.\n\nA bitboard is a specialized bit array data structure commonly used in computer systems that play board games, where each bit corresponds to a game board space or piece.\n\nTo represent the whole board position using bitboards we need one bitboard for each piece type + one bitboard for the empty squares.\n\nLet's see an example with the starting board.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import chess\nboard = chess.Board()\nprint(board.unicode())",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-19T20:46:08.324379Z",
     "start_time": "2025-03-19T20:46:08.298714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "♜ ♞ ♝ ♛ ♚ ♝ ♞ ♜\n",
      "♟ ♟ ♟ ♟ ♟ ♟ ♟ ♟\n",
      "⭘ ⭘ ⭘ ⭘ ⭘ ⭘ ⭘ ⭘\n",
      "⭘ ⭘ ⭘ ⭘ ⭘ ⭘ ⭘ ⭘\n",
      "⭘ ⭘ ⭘ ⭘ ⭘ ⭘ ⭘ ⭘\n",
      "⭘ ⭘ ⭘ ⭘ ⭘ ⭘ ⭘ ⭘\n",
      "♙ ♙ ♙ ♙ ♙ ♙ ♙ ♙\n",
      "♖ ♘ ♗ ♕ ♔ ♗ ♘ ♖\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "def print_bitboard(bitboard):\n    \n    # for each row\n    for i in range(8):\n        \n        # print bitboard of 1s/0s (we have to mirror the bitboard)\n        if i == 0:\n            print(format(bitboard, \"064b\")[8*(i+1)-1::-1])\n        else:\n            print(format(bitboard, \"064b\")[8*(i+1)-1:8*i-1:-1])\n\n# bitboard of pawns\nprint_bitboard(board.pawns)",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-19T20:46:08.853709Z",
     "start_time": "2025-03-19T20:46:08.848055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000\n",
      "11111111\n",
      "00000000\n",
      "00000000\n",
      "00000000\n",
      "00000000\n",
      "11111111\n",
      "00000000\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "# bitboard of kings\nprint_bitboard(board.kings)",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-19T20:46:09.291313Z",
     "start_time": "2025-03-19T20:46:09.286578Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00001000\n",
      "00000000\n",
      "00000000\n",
      "00000000\n",
      "00000000\n",
      "00000000\n",
      "00000000\n",
      "00001000\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": "# bitboard of bishops\nprint_bitboard(board.bishops)",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-19T20:46:09.687642Z",
     "start_time": "2025-03-19T20:46:09.684997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00100100\n",
      "00000000\n",
      "00000000\n",
      "00000000\n",
      "00000000\n",
      "00000000\n",
      "00000000\n",
      "00100100\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": "# bitboard of empty squares\nprint_bitboard(board.occupied ^ 2 ** 64 -1)",
   "metadata": {
    "scrolled": true,
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-19T20:46:10.001139Z",
     "start_time": "2025-03-19T20:46:09.997863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000\n",
      "00000000\n",
      "11111111\n",
      "11111111\n",
      "11111111\n",
      "11111111\n",
      "00000000\n",
      "00000000\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": "## Action Space\nThe Action space must contain all POSSIBLE moves, not VALID moves.\n\nWe decided the number of possible moves as 4096 (64 squares from -> 64 squares to).\n\nOut of this 4096 combinations, not all of them are truly possible (for example no piece can move from square a1 to square b7). But for this exercise we decide to ignore this as it would complicate things.\n\nOut of this set of possible moves, of course only a very small subset of moves can be chosen in a specific state.\n\nTo narrow down these valid moves we are going to apply a masking layer in our Neural Net.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\n\ndef encode_valid_moves(board):\n    \n    valid_moves_dict = {}\n    \n    # for each valid move\n    for move in board.legal_moves:\n        \n        # compute index based on starting square and target square\n        index = 64 * (move.from_square) + (move.to_square)\n        valid_moves_dict[index] = move\n    \n    return valid_moves_dict\n\nvalid_moves_dict = encode_valid_moves(board)\nvalid_moves_dict",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-19T20:46:10.911713Z",
     "start_time": "2025-03-19T20:46:10.879793Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{407: Move.from_uci('g1h3'),\n",
       " 405: Move.from_uci('g1f3'),\n",
       " 82: Move.from_uci('b1c3'),\n",
       " 80: Move.from_uci('b1a3'),\n",
       " 983: Move.from_uci('h2h3'),\n",
       " 918: Move.from_uci('g2g3'),\n",
       " 853: Move.from_uci('f2f3'),\n",
       " 788: Move.from_uci('e2e3'),\n",
       " 723: Move.from_uci('d2d3'),\n",
       " 658: Move.from_uci('c2c3'),\n",
       " 593: Move.from_uci('b2b3'),\n",
       " 528: Move.from_uci('a2a3'),\n",
       " 991: Move.from_uci('h2h4'),\n",
       " 926: Move.from_uci('g2g4'),\n",
       " 861: Move.from_uci('f2f4'),\n",
       " 796: Move.from_uci('e2e4'),\n",
       " 731: Move.from_uci('d2d4'),\n",
       " 666: Move.from_uci('c2c4'),\n",
       " 601: Move.from_uci('b2b4'),\n",
       " 536: Move.from_uci('a2a4')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": "Each one of the 4096 moves represent a move from a square to another. The first move (which is not actually possible) is move from a1 to a1. The second a1 -> b1, the third a1 -> c1, and so on.\n\nThe index grows for the target square from left to right (by 1) and from bottom to top (by 8).\n\nThe index grows for the starting square from left to right (by 64) and from bottom to top (by 512).\n\nWith these moves encoded in this way we can build a mask (of 0s/1s) in order to disable the output layer nodes that correspond to a non-valid move.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def mask_and_valid_moves(board):\n\n    mask = np.zeros((64, 64))\n    valid_moves_dict = {}\n    \n    # for each valid move\n    for move in board.legal_moves:\n        \n        # mask is a matrix\n        mask[move.from_square, move.to_square] = 1\n        \n        # compute index based on starting square and target square\n        index = 64 * (move.from_square) + (move.to_square)\n        \n        valid_moves_dict[index] = move\n    \n    return mask, valid_moves_dict\n\nmask, valid_moves_dict = mask_and_valid_moves(board)\nmask",
   "metadata": {
    "scrolled": true,
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-19T20:46:12.082478Z",
     "start_time": "2025-03-19T20:46:12.063402Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": "## Neural Network architecture\nWe are going to use an hybrid neural network consisting of an initial series of convolutional layers and a following series of fully connected layers.\n\nThe convolutional layers are used to extract features from the \"image\" of the board, the fully connected layers to compute a function that takes this features as input and the move to make as output.\n\nFor the final step we will use a mask layer to reduce the number of selectable moves to only the valid moves.\n\nThe process is quite similar to how a human would act.\n\nA human looks at the board and extract features like piece position, possible attacks, threats from the opponent, possible moves, checks, and so on. And then uses this information to take a decision about which move to make.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### How does a Neural network work?\nA neural network is made of layers and nodes.\n\n<img src=\"https://raw.githubusercontent.com/MandM-DataScience/chess-engine/master/images/colored_neural_network-1200x1443.png\" alt=\"Neural network diagram\" width=250 height=250>\n\nEach node of a layer takes as input the output of the previous layer (or a subset of that) and computes a function on that input (activation function).\n\nEach node has a weight and bias parameters that are used inside that function.\n\nThe output of the nodes of the layer is then passed as input to the next layer and so on, until the final layer that will output the neural net prediction.\n\nThe prediction is then compared to the actual values, computing a loss (or error).\n\nThis error is then backpropagated from the output layer to previous layers, where each node adjust its weight and bias parameters in order to minimize that error.\n\n![Neural network backpropagation](https://raw.githubusercontent.com/MandM-DataScience/chess-engine/master/images/backpropagation.gif)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### How does a convolutional layer work?\nThe layer performs a dot product between two matrices, where one matrix is the set of learnable parameters otherwise known as a kernel, and the other matrix is a subset of the input.\n\n<img src=\"https://raw.githubusercontent.com/MandM-DataScience/chess-engine/master/images/convolution.gif\" alt=\"Convolution\" width=400 height=400>\n\nDuring the forward pass, the kernel slides across the height and width of the image/input. The sliding size of the kernel is called a stride.\n\nThis is used to extract features from subsets of the image to be passed to following layers.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "Let's see how to build this network using PyTorch, a library used to create neural networks in python. https://pytorch.org/",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# This is our final mask layer\nclass MaskLayer(nn.Module):\n\n    def __init__(self):\n        super(MaskLayer, self).__init__()\n    \n    # mask is made of 0s/1s so it will just set to 0 any invalid move\n    def forward(self, x, mask):\n        return torch.mul(x, mask)",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-19T20:46:15.407355Z",
     "start_time": "2025-03-19T20:46:15.398715Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": "BatchNorm2d layers are normalization layers that subtract the batch mean and divide by the batch standard deviation.\n\nIt then applies a scaling factor and an offset to the normalized output. The scaling factor and offset are learned during training and are applied during both training and inference.\n\nThe purpose of these layers is to improve the training process and reduce overfitting by normalizing the input to each node in a layer. This can lead to faster convergence during training and better generalization to new data during inference.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class DQN(nn.Module):\n\n    def __init__(self):\n        \n        super(DQN, self).__init__()\n        \n        # input size = 8 (rows) x 8 (cols) x 16 (bitboards)\n        # - 6 bitboards for white pieces\n        # - 6 bitboards for black pieces\n        # - 1 for empty squares\n        # - 1 for castling rights\n        # - 1 for en passant\n        # - 1 for player\n        \n        # first convolutional layer 8x8x16 => 8x8x32\n        self.conv1 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(32)\n        \n        # second convolutional layer 8x8x32 => 8x8x64 \n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        \n        # third convolutional layer 8x8x64 => 8x8x128 \n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.bn3 = nn.BatchNorm2d(128)\n        \n        # first fully connected layer 8192 => 8192\n        self.fc1 = nn.Linear(128*64, 128*64)\n        \n        # second fully connected layer 8192 => 4096\n        self.fc2 = nn.Linear(128*64, 64*64)\n        \n        # mask is made of 0s/1s so it will just set to 0 any invalid move 4096 => 4096\n        self.mask = MaskLayer()\n\n    def forward(self, x, mask=None, debug=False):\n        \n        # conv1 + bn1 with activation function ReLU\n        x = nn.functional.relu(self.bn1(self.conv1(x)))\n        \n        # conv2 + bn2 with activation function ReLU\n        x = nn.functional.relu(self.bn2(self.conv2(x)))\n        \n        # conv3 + bn3 with activation function ReLU\n        x = nn.functional.relu(self.bn3(self.conv3(x)))\n        \n        # flatten will transform data structure from 3D 8x8x128 to 1D 8192\n        x = nn.Flatten()(x)\n        \n        # fully connected with activation function ReLU\n        x = nn.functional.relu(self.fc1(x))\n        \n        # fully connected WITHOUT ReLU (we want to keep negative values for our output layer)\n        x = self.fc2(x)\n        \n        # if we have a mask we apply it to set to 0 all invalid moves\n        if mask is not None:\n            x = self.mask(x, mask)\n            \n        return x",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-19T20:46:17.464278Z",
     "start_time": "2025-03-19T20:46:17.453063Z"
    }
   },
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": "A rectified linear unit (ReLU) is an activation function that introduces the property of non-linearity to a neural network model and solves the vanishing gradients issue.\n\nIt is one of the most popular activation functions and it works like a floor function, max(0, output).\n\nhttps://en.wikipedia.org/wiki/Rectifier_(neural_networks)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Reward signal\nThe main signals should be the ones regarding the result of a match: +1 if win, -1 if lose, 0 if draw.\n\nHowever, it is better to introduce intermediate signals, so that we can \"teach\" the agent also during a single game and not only at the end.\n\nFor example a small reward if by making a move the position improves and a lower score if the position worsen.\n\nKeep in mind that to assess the position score in the middle of a game we would need something \"external\" from the model! In our case we will use the stockfish analyse function.\n\nIt is definitevely possible to train a model without any kind of human or other-engine intervention, and just using the match result rewards. But it will probably take longer.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import chess\n",
    "import chess.engine\n",
    "import random\n",
    "\n",
    "# for now our agent choose a random move\n",
    "def agent_choose_move(board):\n",
    "    return random.choice(list(board.legal_moves))\n",
    "\n",
    "# Create a chess board\n",
    "board = chess.Board()\n",
    "\n",
    "# Create a stockfish engine instance\n",
    "stockfish = chess.engine.SimpleEngine.popen_uci(\"/Users/simarjeetss529/Desktop/saar/uni/semesters/spr 25/cpts 540 ai/project/CPT_S-440/stockfish/stockfish-macos-m1-apple-silicon\")\n",
    "\n",
    "# Analyse starting board with stockfish\n",
    "board_score_before = stockfish.analyse(board=board, limit=chess.engine.Limit(depth=5))\\\n",
    "    ['score'].relative.score(mate_score=10000)\n",
    "\n",
    "# Agent choose move\n",
    "move = agent_choose_move(board)\n",
    "board.push(move)\n",
    "\n",
    "# Make random move for black\n",
    "board.push(random.choice(list(board.legal_moves)))\n",
    "\n",
    "# Analyse final board with stockfish\n",
    "board_score_after = stockfish.analyse(board=board, limit=chess.engine.Limit(depth=5))\\\n",
    "    ['score'].relative.score(mate_score=10000)\n",
    "\n",
    "# Divide by 100 to transform to centipawn to pawn score and subtract 0.01 to penalize the agent for each move. \n",
    "# We want to win as fast as possible ;)\n",
    "reward = board_score_after/100 - board_score_before/100 - 0.01"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-19T20:47:56.938853Z",
     "start_time": "2025-03-19T20:47:50.605491Z"
    }
   },
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": "print(board.unicode())",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-19T20:47:58.797310Z",
     "start_time": "2025-03-19T20:47:58.793121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "♜ ♞ ♝ ♛ ♚ ♝ ♞ ♜\n",
      "⭘ ♟ ♟ ♟ ♟ ♟ ♟ ♟\n",
      "♟ ⭘ ⭘ ⭘ ⭘ ⭘ ⭘ ⭘\n",
      "⭘ ⭘ ⭘ ⭘ ⭘ ⭘ ⭘ ⭘\n",
      "⭘ ♙ ⭘ ⭘ ⭘ ⭘ ⭘ ⭘\n",
      "⭘ ⭘ ⭘ ⭘ ⭘ ⭘ ⭘ ⭘\n",
      "♙ ⭘ ♙ ♙ ♙ ♙ ♙ ♙\n",
      "♖ ♘ ♗ ♕ ♔ ♗ ♘ ♖\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "source": "move",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-19T20:48:04.884750Z",
     "start_time": "2025-03-19T20:48:04.874568Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Move.from_uci('b2b4')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "source": "reward",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-19T20:48:06.055602Z",
     "start_time": "2025-03-19T20:48:06.046891Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.18"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": "## Exploration strategy\nIt is crucial to design the agent with an exploration strategy that will allow it to explore new moves instead of picking always the ones that temporarily thinks as best.\n\nOtherwise, the agent will become stuck in some local minimum instead of getting to the best possible solution.\n\nA simple exploration strategy is called epsilon-greedy.\n\nWe have a parameter called epsilon that ranges between 0 and 1 and represent the exploration probability.\n\nAt the beginning of the training the epsilon is set to a high value, like 1 and it is progressively reduced.\n\nFor each step the agent takes a random action with probability epsilon and the \"best move\" with probability 1 - epsilon.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Exploration rate\nepsilon = 1\nepsilon_decay = 0.995\nepsilon_min = 0.01\n\n# choose random with probability epsilon\nif random.uniform(0, 1) <= epsilon:\n    move = random.choice(list(board.legal_moves))\n    \n# choose best move with probability 1 - epsilon\nelse:\n    move = agent_choose_move(board)\n\n# reduce exploration rate after each step\nepsilon = max(epsilon * epsilon_decay, epsilon_min)\nepsilon",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-03-19T20:48:13.002688Z",
     "start_time": "2025-03-19T20:48:12.993585Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.995"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": "## Let's build the agent",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 31,
   "source": [
    "import os\n",
    "\n",
    "class ChessAgent:\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, input_model_path=None):\n",
    "\n",
    "        # Exploration parameters\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "        # Training parameters\n",
    "        self.gamma = 0.5 # tells the agent whether to prefer long term rewards or immediate rewards. 0 = greedy, 1 = long term\n",
    "        self.learning_rate = 1e-03 # how fast the network updates its weights\n",
    "        self.MEMORY_SIZE = 512 # how many steps/moves/samples to store. It is used for training (experience replay) \n",
    "        self.MAX_PRIORITY = 1e+06 # max priority for a sample in memory. The higher the priority, the more likely the sample will be included in training\n",
    "        self.memory = [] # memory data structure\n",
    "        self.batch_size = 16 # how many sample to include in a training step        \n",
    "        \n",
    "        self.policy_net = DQN()\n",
    "        \n",
    "        # Load trained model if exists\n",
    "        if input_model_path is not None and os.path.exists(input_model_path):\n",
    "            self.policy_net.load_state_dict(torch.load(input_model_path))    \n",
    "\n",
    "        # We use mean squared error as our loss function\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        \n",
    "        # Adam optimizer provides adaptive learning rate and a momentum-based approach that can help the neural network \n",
    "        # learn faster and converge more quickly towards the optimal set of parameters that minimize the cost or loss function\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    \n",
    "    # Convert board into a 3D np.array of 16 bitboards\n",
    "    def convert_state(self, board):\n",
    "        \n",
    "        # dictionary to store bitboards\n",
    "        piece_bitboards = {}\n",
    "        \n",
    "        # for each color (white, black)\n",
    "        for color in chess.COLORS:\n",
    "            \n",
    "            # for each piece type (pawn, bishop, knigh, rook, queen, kinb)\n",
    "            for piece_type in chess.PIECE_TYPES:\n",
    "                v = board.pieces_mask(piece_type, color)\n",
    "                symbol = chess.piece_symbol(piece_type)\n",
    "                i = symbol.upper() if color else symbol\n",
    "                piece_bitboards[i] = v\n",
    "\n",
    "        # empty bitboard\n",
    "        piece_bitboards['-'] = board.occupied ^ 2 ** 64 - 1\n",
    "        \n",
    "        # player bitboard (full 1s if player is white, full 0s otherwise)\n",
    "        player = 2 ** 64 - 1 if board.turn else 0\n",
    "\n",
    "        # castling_rights bitboard\n",
    "        castling_rights = board.castling_rights\n",
    "\n",
    "        # en passant bitboard\n",
    "        en_passant = 0\n",
    "        ep = board.ep_square\n",
    "        if ep is not None:\n",
    "            en_passant |= (1 << ep)\n",
    "\n",
    "        # bitboards (16) = 12 for pieces, 1 for empty squares, 1 for player, 1 for castling rights, 1 for en passant\n",
    "        bitboards = [b for b in piece_bitboards.values()] + [player] + [castling_rights] + [en_passant]\n",
    "\n",
    "        # for each bitboard transform integet into a matrix of 1s and 0s\n",
    "        # reshape in 3D format (16 x 8 x 8)\n",
    "        bitarray = np.array([\n",
    "            np.array([(bitboard >> i & 1) for i in range(64)])\n",
    "            for bitboard in bitboards\n",
    "        ]).reshape((16, 8, 8))\n",
    "\n",
    "        return bitarray\n",
    "\n",
    "    \n",
    "    # get the move index out of the 4096 possible moves, as explained before\n",
    "    def get_move_index(self, move):\n",
    "        index = 64 * (move.from_square) + (move.to_square)\n",
    "        return index\n",
    "\n",
    "    \n",
    "    # returns mask of valid moves (out of 4096) + the dictionary with the valid moves and their indexes\n",
    "    def mask_and_valid_moves(self, board):\n",
    "\n",
    "        mask = np.zeros((64, 64))\n",
    "        valid_moves_dict = {}\n",
    "        \n",
    "        for move in board.legal_moves:\n",
    "            mask[move.from_square, move.to_square] = 1\n",
    "            valid_moves_dict[self.get_move_index(move)] = move\n",
    "        \n",
    "        # mask is flatten and returned as a PyTorch tensor\n",
    "        # a tensor is just a vector optimized for derivatives computation, used in PyTorch neural nets\n",
    "        return torch.from_numpy(mask.flatten()), valid_moves_dict\n",
    "\n",
    "    \n",
    "    # insert a step/move/sample into memory to be used in training as experience replay\n",
    "    def remember(self, priority, state, action, reward, next_state, done, valid_moves, next_valid_moves):\n",
    "\n",
    "        # if memory is full, we delete the least priority element\n",
    "        if len(self.memory) >= self.MEMORY_SIZE:\n",
    "            \n",
    "            min_value = self.MAX_PRIORITY\n",
    "            min_index = 0\n",
    "            \n",
    "            for i,n in enumerate(self.memory):\n",
    "                \n",
    "                # priority is stored in the first position of the tuple\n",
    "                if n[0] < min_value:\n",
    "                    min_value = n[0]\n",
    "                    min_index = i\n",
    "            \n",
    "            del self.memory[min_index]\n",
    "\n",
    "        self.memory.append((priority, state, action, reward, next_state, done, valid_moves, next_valid_moves))\n",
    "\n",
    "    \n",
    "    # Take a board as input and return a valid move defined as tuple (start square, end square)\n",
    "    def select_action(self, board, best_move):\n",
    "\n",
    "        # convert board into the 16 bitboards\n",
    "        bit_state = self.convert_state(board)\n",
    "        \n",
    "        # get valid moves\n",
    "        valid_moves_tensor, valid_move_dict = self.mask_and_valid_moves(board)\n",
    "        \n",
    "        # with probability epsilon = Explore\n",
    "        if random.uniform(0, 1) <= self.epsilon:\n",
    "            \n",
    "            r = random.uniform(0, 1)\n",
    "            \n",
    "            # inside exploration with probability 10% choose best move (as computed by stockfish)\n",
    "            if r <= 0.1:\n",
    "                chosen_move = best_move\n",
    "            \n",
    "            # with probability 90% choose a random move\n",
    "            else:\n",
    "                chosen_move = random.choice(list(valid_move_dict.values()))\n",
    "        \n",
    "        # with probability 1 - epsilon = Exploit\n",
    "        else:\n",
    "            \n",
    "            # during inference we don't need to compute gradients\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # transform our 16 bitboards in a tensor of shape 1 x 16 x 8 x 8\n",
    "                tensor = torch.from_numpy(bit_state).float().unsqueeze(0)\n",
    "                \n",
    "                # predict rewards for each valid move in the current state. valid_moves_tensor is the mask!\n",
    "                policy_values = self.policy_net(tensor, valid_moves_tensor)\n",
    "                \n",
    "                # take the move index with the highest predicted reward\n",
    "                chosen_move_index = int(policy_values.max(1)[1].view(1,1))\n",
    "                \n",
    "                # if move is valid:\n",
    "                if chosen_move_index in valid_move_dict:\n",
    "                    chosen_move = valid_move_dict[chosen_move_index]\n",
    "                    \n",
    "                # if move is NOT valid, choose random move\n",
    "                # this can happen if all valid moves have predicted values 0 or negative\n",
    "                else:\n",
    "                    chosen_move = random.choice(list(board.legal_moves))\n",
    "\n",
    "        return self.get_move_index(chosen_move), chosen_move, bit_state, valid_moves_tensor\n",
    "\n",
    "    \n",
    "    # Decay epsilon (exploration rate)\n",
    "    def adaptiveEGreedy(self):\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "\n",
    "        \n",
    "    # Save trained model\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.policy_net.state_dict(), path)"
   ]
  }
 ]
}
